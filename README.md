# Projeto de Modelagem Dimensional com Spark SQL

Este reposit√≥rio cont√©m a entrega do desafio t√©cnico baseado em modelagem dimensional com PySpark (utilizando SQL), realizado no ambiente do Google Colab.

## üìÅ Estrutura dos Arquivos

```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ dim_cliente.csv
‚îÇ   ‚îú‚îÄ‚îÄ dim_produto.csv
‚îÇ   ‚îú‚îÄ‚îÄ dim_data.csv
‚îÇ   ‚îî‚îÄ‚îÄ fato_vendas.csv
‚îî‚îÄ‚îÄ notebook/
    ‚îî‚îÄ‚îÄ gaudium_test.ipynb
```

## üß± Tecnologias Utilizadas

- Apache Spark (3.x)
- Google Colab
- PySpark (com interface SQL)
- Python 3.8+

Observa√ß√µes: Google Colab foi escolhido pela facilidade de, na sua vers√£o gratuita, integrar com GitHUb e organizar arquivos diretamente nos diret√≥rios.

## üß† Descri√ß√£o das Tabelas

### `dim_cliente`
Cont√©m informa√ß√µes de clientes e suas localidades.

| Coluna         | Tipo    | Descri√ß√£o                          |
|----------------|---------|------------------------------------|
| id_cliente     | inteiro | Chave substituta do cliente        |
| nome_cliente   | texto   | Nome do cliente                    |
| cidade         | texto   | Cidade do cliente                  |
| estado         | texto   | Estado do cliente                  |

### `dim_produto`
Cont√©m produtos, categorias e fabricantes.

| Coluna         | Tipo    | Descri√ß√£o                          |
|----------------|---------|------------------------------------|
| id_produto     | inteiro | Chave substituta do produto        |
| nome_produto   | texto   | Nome do produto                    |
| categoria      | texto   | Categoria de produto               |
| fabricante     | texto   | Fabricante                         |

### `dim_data`
Calend√°rio simplificado com datas √∫nicas.

| Coluna         | Tipo    | Descri√ß√£o                          |
|----------------|---------|------------------------------------|
| id_data        | inteiro | Chave substituta da data           |
| data           | data    | Data da venda                      |
| ano            | inteiro | Ano                                |
| mes            | inteiro | M√™s                                |
| dia            | inteiro | Dia do m√™s                         |
| dia_semana     | texto   | Dia da semana (em portugu√™s)       |

### `fato_vendas`
Tabela fato com m√©tricas de vendas.

| Coluna         | Tipo          | Descri√ß√£o                              |
|----------------|---------------|----------------------------------------|
| id_fato        | inteiro       | Chave t√©cnica da linha de venda        |
| qtd_vendida    | inteiro       | Quantidade vendida                     |
| valor_total    | decimal(10,2) | Valor total da venda                   |
| id_cliente     | inteiro       | Chave Estrangeira para `dim_cliente`   |
| id_produto     | inteiro       | Chave Estrangeira para `dim_produto`   |
| id_data        | inteiro       | Chave Estrangeira para `dim_data`      |

## üß† Explica√ß√£o sobre as Decis√µes Tomadas

### Premissas
- As manipula√ß√µes de dados foram todas feitas em Spark SQL, como solicitado, com exce√ß√£o de uma mudan√ßa inicial de tipos de dados que ser√° explicada nos itens seguintes.
- Como solicitado tamb√©m, as tabelas foram dimensionadas no esquema estrela, menos granulado que o modelo floco de neve, mas julgando pelo volume de dados e quantidade de categorias em cada dimens√£o, o esquema estrela era sim o mais correto a ser usado, pelo menos em um primeiro momento, por n√£o haver tanta cardinalidade nas dimens√µes e porque uma menor quantidade de tabelas dimens√µes √© sempre que poss√≠vel desej√°vel para facilitar a manuten√ß√£o e diminuir a quantidade de joins, gerando queries mais r√°pidas. Caso o volume de dados e categorias mudasse, seria necess√°rio o uso do esquema floco de neve.
- Optou-se por cada tabela gerada ser separada em uma c√©lula pr√≥pria para visualiza√ß√£o de output e facilidade de debugging.
- A visualiza√ß√£o foi feita atrav√©s de 'display(HTML())' para uma tabula√ß√£o melhor que um simples '.show()'. A prefer√™ncia deste candidato √© por usar convers√£o a Pandas dentro de display, quando em ambiente diferente do Databricks, devido ao seu visual mais agrad√°vel e interativo, mas achei importante para o teste me ater o m√°ximo poss√≠vel as ferramentas solicitadas.

### C√©lula 1
- Setup do ambiente Spark dentro do Colab.

### C√©lula 2
- Importa√ß√£o de bibliotecas, dos dados brutos e mudan√ßa dos tipos das colunas data, qtd_vendida e valor_total.
- Foi decidida realizar a mudan√ßa conjunta com a importa√ß√£o dos dados brutos com o objetivo de economizar linhas de c√≥digo que seriam repetidas mais de uma vez posteriormente, principalmente em rela√ß√£o a coluna data, que seria usada como coordenada de JOIN.
- A coluna data foi transformada em date type para permitir o uso de fun√ß√µes como YEAR() que s√≥ funcionam com date type.
- A coluna qtd_vendida foi transformada em inteiro para permitir que eventualmente possam ser feitos.
- A coluna valor_total foi transformada em decimal pelo mesmo motivo de c√°lculo, mas escolhido um formato mais representativo de valores monet√°rios.
- Foi feita anteriormente uma devida explora√ß√£o dos dados e foi verificada a inexist√™ncia de nulos e duplicados e investigado o volume dos dados.

### C√©lula 3
- Constru√ß√£o da tabela dim_cliente.
- Cidade e estado foram colocados juntos na tabela dimens√£o de clientes para garantir a unicidade destes, j√° que eventualmente poderia haver clientes com o mesmo nome, mas que poderiam ser distinguidos por suas localidades.

### C√©lula 4
- Constru√ß√£o da tabela dim_produto.
- Categoria e fabricante tamb√©m foram colocados juntos com nome_produto tamb√©m para garantir sua unicidade, pois se a tabela crescesse poderia ser poss√≠vel ter um mesmo nome_produto, mas de marca diferente.

### C√©lula 5
- Constru√ß√£o da tabela dim_data.
- O padr√£o de cria√ß√£o de colunas seguiu o modelo cl√°ssico separando dia, m√™s e ano, abrindo a possibilidade de filtragem mais acess√≠vel posterior.
- O uso da CTE se deve, n√£o s√≥ a sua agilidade, mas ao fato de que identifiquei a necessidade de fazer uma tradu√ß√£o dos dias da semana, j√° que o Spark SQL n√£o possui fun√ß√£o ou par√¢metro que permita a direta convers√£o para os dias da semana em portugu√™s, e √© necess√°rio manter tais tabelas mais leg√≠veis n√£o s√≥ aos engenheiros, mas outros setores que possam utiliz√°-la.

### C√©lula 6
- Constru√ß√£o da tabela fato_vendas.
- O uso da CTE se deve a necessidade de usar DISTINCT antes de se criar a chave prim√°ria, evitando ids eliminados por terem sido atribu√≠dos a linhas duplicadas.
- N√£o h√° linhas duplicadas nos dados brutos fornecidos, principalmente quando se leva em conta a quantidade de itens vendidos e o pre√ßo final. √â n√≠tida que a inten√ß√£o da tabela √© que todas as vendas de um certo produto, para um certo cliente, em uma certa data sejam todas somadas e colocadas em apenas uma linha. Todavia, se porventura houvessem linhas com valores iguais, poder√≠amos seguramente interpretar, dada a natureza da coluna qte_vendida, que tal linha a mais n√£o deveria ser considerada como um registro separado genu√≠no a ser posteriormente envolvido em c√°lculos, mas sim um registro err√¥neo realmente duplicado. Pensando nisso que se decidiu por criar uma query que fosse a prova de posteriores erros como este. Assim se faz a necessidade da CTE e do DISTINCT antes da query principal.

### C√©lula 7
- Exporta√ß√£o de cada uma das tabelas para arquivos .csv.

## üöÄ Como Reproduzir

1. Acesse o notebook [`star_spark_sql.ipynb`](notebook/star_spark_sql.ipynb)
2. Execute o notebook no Google Colab
3. Fa√ßa upload de `dados_brutos.csv` ao ser solicitado
4. Exporte os arquivos `.csv` ou `.zip` no final do processo

---

## ‚úçÔ∏è Autor

Higor Moretti Pereira
